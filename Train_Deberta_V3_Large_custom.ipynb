{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kJEZuO2FA3VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate==0.20.3"
      ],
      "metadata": {
        "id": "CkiYNejJaREC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "\n",
        "from typing import Optional, Union\n",
        "import pandas as pd, numpy as np, torch\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "\n",
        "VER=2\n",
        "# TRAIN WITH SUBSET OF 60K\n",
        "NUM_TRAIN_SAMPLES = 53_206\n",
        "# PARAMETER EFFICIENT FINE TUNING\n",
        "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
        "USE_PEFT = False\n",
        "# NUMBER OF LAYERS TO FREEZE\n",
        "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
        "FREEZE_LAYERS = 0  #18\n",
        "# BOOLEAN TO FREEZE EMBEDDINGS\n",
        "FREEZE_EMBEDDINGS = True\n",
        "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
        "MAX_INPUT = 786    #256\n",
        "# HUGGING FACE MODEL\n",
        "MODEL = 'microsoft/deberta-v3-large'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:09.222184Z",
          "iopub.execute_input": "2023-09-07T20:17:09.222848Z",
          "iopub.status.idle": "2023-09-07T20:17:24.749198Z",
          "shell.execute_reply.started": "2023-09-07T20:17:09.222809Z",
          "shell.execute_reply": "2023-09-07T20:17:24.747157Z"
        },
        "trusted": true,
        "id": "jwpkhfXkA13V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/train_with_context2.csv')\n",
        "extra = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/val_context.csv')\n",
        "extra_2 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/New_val2.csv')\n",
        "extra_2 = extra_2[:300]\n",
        "df_valid = pd.concat([df_valid, extra, extra_2])\n",
        "df_valid.reset_index(drop=True, inplace=True)\n",
        "del df_valid['Unnamed: 0']\n",
        "del df_valid['source']\n",
        "df_valid.drop_duplicates(subset='prompt', inplace=True)\n",
        "df_valid = df_valid.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print('Validation data size:', df_valid.shape )\n",
        "df_valid.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:24.751484Z",
          "iopub.execute_input": "2023-09-07T20:17:24.752140Z",
          "iopub.status.idle": "2023-09-07T20:17:24.833084Z",
          "shell.execute_reply.started": "2023-09-07T20:17:24.752084Z",
          "shell.execute_reply": "2023-09-07T20:17:24.832129Z"
        },
        "trusted": true,
        "id": "kyS8CXVIA13Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/New_train2.csv')\n",
        "#df_train_2 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/train_add_1.csv')\n",
        "#df_train_3 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/train_add_2.csv')\n",
        "#df_train_4 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/train_add_3.csv')\n",
        "#df_train = pd.concat([df_train, df_train_2, df_train_3, df_train_4])\n",
        "#df_train = df_train.drop(columns=\"source\")\n",
        "df_train.drop_duplicates(subset='prompt', inplace=True)\n",
        "df_train = df_train[~df_train['prompt'].isin(df_valid['prompt'])]\n",
        "df_train = df_train.fillna('').sample(NUM_TRAIN_SAMPLES)\n",
        "del df_train['Unnamed: 0']\n",
        "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "df_train = df_train[['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer']]\n",
        "\n",
        "print('Train data size:', df_train.shape )\n",
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:24.834730Z",
          "iopub.execute_input": "2023-09-07T20:17:24.835347Z",
          "iopub.status.idle": "2023-09-07T20:17:32.649732Z",
          "shell.execute_reply.started": "2023-09-07T20:17:24.835314Z",
          "shell.execute_reply": "2023-09-07T20:17:32.648645Z"
        },
        "trusted": true,
        "id": "qY_45xsaA13Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader\n",
        "Code is from Radek's notebook [here][1] with modifications to the tokenization process.\n",
        "\n",
        "[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training"
      ],
      "metadata": {
        "id": "jY5bylXqA13Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
        "index_to_option = {v: k for k,v in option_to_index.items()}\n",
        "\n",
        "def preprocess(example):\n",
        "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
        "    #second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
        "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + (example[option] if option in example else \"\") + \" [SEP]\" for option in 'ABCDE']\n",
        "\n",
        "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n",
        "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
        "    tokenized_example['label'] = option_to_index[example['answer']]\n",
        "\n",
        "    return tokenized_example\n",
        "\n",
        "#second is only a test\n",
        "def preprocess(example):\n",
        "    first_sentence = [\"[CLS] \" + example.get('context', '')] * 5  # Use get() to provide a default value for 'context'\n",
        "    second_sentences = []\n",
        "\n",
        "    prompt = example.get('prompt', '')  # Use get() to provide a default value for 'prompt'\n",
        "\n",
        "    for option in 'ABCDE':\n",
        "        option_text = example.get(option, '')  # Use get() to provide a default value for each option\n",
        "        if option_text:\n",
        "            second_sentences.append(\" #### \" + prompt + \" [SEP] \" + option_text + \" [SEP]\")\n",
        "        else:\n",
        "            second_sentences.append(\"\")  # Use an empty string for missing options\n",
        "\n",
        "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', max_length=MAX_INPUT, add_special_tokens=False)\n",
        "    tokenized_example['label'] = option_to_index[example['answer']]\n",
        "    return tokenized_example\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0]['input_ids'])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:32.651323Z",
          "iopub.execute_input": "2023-09-07T20:17:32.651747Z",
          "iopub.status.idle": "2023-09-07T20:17:32.665816Z",
          "shell.execute_reply.started": "2023-09-07T20:17:32.651714Z",
          "shell.execute_reply": "2023-09-07T20:17:32.664764Z"
        },
        "trusted": true,
        "id": "OrZbPkzfA13b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "dataset_valid = Dataset.from_pandas(df_valid)\n",
        "dataset = Dataset.from_pandas(df_train)\n",
        "#dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:32.670137Z",
          "iopub.execute_input": "2023-09-07T20:17:32.670934Z",
          "iopub.status.idle": "2023-09-07T20:17:34.907329Z",
          "shell.execute_reply.started": "2023-09-07T20:17:32.670898Z",
          "shell.execute_reply": "2023-09-07T20:17:34.906329Z"
        },
        "trusted": true,
        "id": "z1PCK8RpA13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
        "\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:17:34.909125Z",
          "iopub.execute_input": "2023-09-07T20:17:34.909839Z",
          "iopub.status.idle": "2023-09-07T20:22:54.554641Z",
          "shell.execute_reply.started": "2023-09-07T20:17:34.909802Z",
          "shell.execute_reply": "2023-09-07T20:22:54.553646Z"
        },
        "trusted": true,
        "id": "o-4UviyPA13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model\n",
        "We will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1]. We can optionally use PEFT to accelerate training and use less memory. However i have noticed that validation accuracy is less. (Note that PEFT requires us to use 1xP100 not 2xT4 GPU. I'm not sure why). We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n",
        "\n",
        "[1]: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "KuoDmnnfA13d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMultipleChoice.from_pretrained(MODEL)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:22:54.556219Z",
          "iopub.execute_input": "2023-09-07T20:22:54.556829Z",
          "iopub.status.idle": "2023-09-07T20:23:03.399979Z",
          "shell.execute_reply.started": "2023-09-07T20:22:54.556793Z",
          "shell.execute_reply": "2023-09-07T20:23:03.398905Z"
        },
        "trusted": true,
        "id": "cyRmsvKeA13d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\n",
        "if USE_PEFT:\n",
        "    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.401395Z",
          "iopub.execute_input": "2023-09-07T20:23:03.401861Z",
          "iopub.status.idle": "2023-09-07T20:23:03.407326Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.401826Z",
          "shell.execute_reply": "2023-09-07T20:23:03.406349Z"
        },
        "trusted": true,
        "id": "tOIPMRwHA13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_PEFT:\n",
        "    print('We are using PEFT.')\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    peft_config = LoraConfig(\n",
        "        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1,\n",
        "        bias=\"none\", inference_mode=False,\n",
        "        target_modules=[\"query_proj\", \"value_proj\"],\n",
        "        modules_to_save=['classifier','pooler'],\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.408852Z",
          "iopub.execute_input": "2023-09-07T20:23:03.409528Z",
          "iopub.status.idle": "2023-09-07T20:23:03.543352Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.409491Z",
          "shell.execute_reply": "2023-09-07T20:23:03.541988Z"
        },
        "trusted": true,
        "id": "pR-E30f6A13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FREEZE_EMBEDDINGS:\n",
        "    print('Freezing embeddings.')\n",
        "    for param in model.deberta.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "if FREEZE_LAYERS>0:\n",
        "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
        "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.545200Z",
          "iopub.execute_input": "2023-09-07T20:23:03.545755Z",
          "iopub.status.idle": "2023-09-07T20:23:03.555941Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.545717Z",
          "shell.execute_reply": "2023-09-07T20:23:03.554901Z"
        },
        "trusted": true,
        "id": "wvvRM9DDA13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAP@3 Metric\n",
        "The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n",
        "\n",
        "[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"
      ],
      "metadata": {
        "id": "raJInRSvA13f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_at_3(predictions, labels):\n",
        "    map_sum = 0\n",
        "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
        "    for x,y in zip(pred,labels):\n",
        "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
        "        map_sum += np.sum(z)\n",
        "    return map_sum / len(predictions)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = p.predictions.tolist()\n",
        "    labels = p.label_ids.tolist()\n",
        "    return {\"map@3\": map_at_3(predictions, labels)}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.557393Z",
          "iopub.execute_input": "2023-09-07T20:23:03.558450Z",
          "iopub.status.idle": "2023-09-07T20:23:03.566596Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.558411Z",
          "shell.execute_reply": "2023-09-07T20:23:03.565509Z"
        },
        "trusted": true,
        "id": "8_Auq4tCA13f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Save\n",
        "We will now train and save our model using Hugging Face's easy to use trainer. By adjusting the parameters in this notebook, we can achieve `CV MAP@3 = 0.915+` and corresponding single model `LB MAP@3 = 0.830+` wow!\n",
        "\n",
        "In we run this notebook outside of Kaggle then we can train longer and with more RAM. If we run this notebook on Kaggle, then we need to use tricks to train models efficiently. Here are some ideas:\n",
        "* use fp16 (this speeds up T4 not P100)\n",
        "* use gradient_accumlation_steps (this simulates larger batch sizes)\n",
        "* use gradient_checkpointing (this uses disk to save RAM)\n",
        "* use 2xT4 instead of 1xP100 (this doubles GPUs)\n",
        "* freeze model embeddings (this reduces weights to train)\n",
        "* freeze some model layers (this reduces weights to train)\n",
        "* use PEFT (this reduces weights to train)\n",
        "* increase LR and decrease epochs (this reduces work)\n",
        "* use smaller models (this reduces weights to train)"
      ],
      "metadata": {
        "id": "YVnBCrGlA13f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    warmup_ratio=0.17,\n",
        "    learning_rate=2e-6,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=2,\n",
        "    report_to='none',\n",
        "    output_dir = f'./checkpoints_{VER}',\n",
        "    overwrite_output_dir=True,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=32,\n",
        "    logging_steps=25,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='map@3',\n",
        "    lr_scheduler_type='cosine',\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.569285Z",
          "iopub.execute_input": "2023-09-07T20:23:03.569888Z",
          "iopub.status.idle": "2023-09-07T20:23:03.648982Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.569855Z",
          "shell.execute_reply": "2023-09-07T20:23:03.647945Z"
        },
        "trusted": true,
        "id": "_iVekLTzA13f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset_valid,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=20)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(f'./model_v{VER}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:03.650464Z",
          "iopub.execute_input": "2023-09-07T20:23:03.650850Z",
          "iopub.status.idle": "2023-09-07T20:23:17.937200Z",
          "shell.execute_reply.started": "2023-09-07T20:23:03.650817Z",
          "shell.execute_reply": "2023-09-07T20:23:17.935076Z"
        },
        "trusted": true,
        "id": "PvE8Ip-wA13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Saved Model\n",
        "During training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly."
      ],
      "metadata": {
        "id": "ZNtUdEElA13g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#del model, trainer\n",
        "if USE_PEFT:\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n",
        "    model.load_state_dict(checkpoint)\n",
        "else:\n",
        "    model = AutoModelForMultipleChoice.from_pretrained(f'./model_v{VER}')\n",
        "\n",
        "trainer = Trainer(model=model)\n",
        "trainer.save_model(f'/content/drive/MyDrive/ColabNotebooks/llm_train/Output/model_v{VER}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:17.938669Z",
          "iopub.status.idle": "2023-09-07T20:23:17.939503Z",
          "shell.execute_reply.started": "2023-09-07T20:23:17.939253Z",
          "shell.execute_reply": "2023-09-07T20:23:17.939278Z"
        },
        "trusted": true,
        "id": "d_tB5RkJA13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/train_with_context2.csv')\n",
        "\n",
        "tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n",
        "        preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n",
        "\n",
        "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
        "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
        "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
        "predictions_as_string = test_df['prediction'] = [\n",
        "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:17.941009Z",
          "iopub.status.idle": "2023-09-07T20:23:17.941821Z",
          "shell.execute_reply.started": "2023-09-07T20:23:17.941578Z",
          "shell.execute_reply": "2023-09-07T20:23:17.941602Z"
        },
        "trusted": true,
        "id": "WULNhT0QA13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Validation Score"
      ],
      "metadata": {
        "id": "X4Y2lwdBA13g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
        "import numpy as np\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Precision at k\"\"\"\n",
        "    assert k <= len(r)\n",
        "    assert k != 0\n",
        "    return sum(int(x) for x in r[:k]) / k\n",
        "\n",
        "def MAP_at_3(predictions, true_items):\n",
        "    \"\"\"Score is mean average precision at 3\"\"\"\n",
        "    U = len(predictions)\n",
        "    map_at_3 = 0.0\n",
        "    for u in range(U):\n",
        "        user_preds = predictions[u].split()\n",
        "        user_true = true_items[u]\n",
        "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
        "        for k in range(min(len(user_preds), 3)):\n",
        "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
        "    return map_at_3 / U"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:17.943384Z",
          "iopub.status.idle": "2023-09-07T20:23:17.944219Z",
          "shell.execute_reply.started": "2023-09-07T20:23:17.943964Z",
          "shell.execute_reply": "2023-09-07T20:23:17.943987Z"
        },
        "trusted": true,
        "id": "nRlE391gA13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
        "print( 'CV MAP@3 =',m )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-07T20:23:17.945550Z",
          "iopub.status.idle": "2023-09-07T20:23:17.946317Z",
          "shell.execute_reply.started": "2023-09-07T20:23:17.946065Z",
          "shell.execute_reply": "2023-09-07T20:23:17.946088Z"
        },
        "trusted": true,
        "id": "6doaM8D2A13g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.to_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/Output/df_valid_from_model.csv')\n",
        "df_train.to_csv('/content/drive/MyDrive/ColabNotebooks/llm_train/Output/df_train_from_model.csv')"
      ],
      "metadata": {
        "id": "4p5ZHpwr_rCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}